#===Импорт необходимых библиотек
import numpy as np
import matplotlib.pyplot as plt


#===Вычиление значения функции потерь (сигмоидная функция)
def func_sigmoid(w_param, x_param, y_param):
    M = np.dot(w_param, x_param) * y_param                #вычисление отступа
    return 2 / (1 + np.exp(M))                            #значение сигмоидной функции


#===Вічисление производной сигмоидной функции потерь по вектору w
def func_dsigmoid(w_param, x_param, y_param):
    M = np.dot(w_param, x_param) * y_param
    return -2 * (1 + np.exp(M)) ** (-2) * np.exp(M) * x_param * y_param


#==========================================Основная функция

# обучающая выборка с тремя признаками (третий - константа +1)
data_x = [[3.0, 1.3], [3.4, 1.6], [3.4, 0.4], [3.7, 0.2], [3.5, 0.2], [3.4, 0.2], [3.4, 0.4], [3.9, 0.4], [3.4, 0.3],
          [3.2, 0.2], [2.8, 1.3], [3.5, 0.3], [2.4, 1.0], [3.0, 0.1], [3.6, 0.2], [3.2, 0.2], [2.9, 0.2], [2.9, 1.3],
          [2.3, 1.3], [3.8, 0.2], [3.2, 1.5], [2.3, 1.0], [3.0, 1.7], [3.3, 0.2], [3.4, 0.2], [3.8, 0.3], [2.0, 1.0],
          [3.1, 0.2], [2.5, 1.3], [2.4, 1.1], [3.2, 0.2], [2.2, 1.0], [3.1, 1.4], [3.0, 0.2], [3.0, 0.2], [3.4, 0.2],
          [3.7, 0.2], [2.8, 1.2], [2.9, 1.4], [4.0, 0.2], [3.2, 1.4], [3.2, 0.2], [2.9, 1.3], [2.9, 1.3], [3.5, 0.2],
          [3.3, 1.6], [2.9, 1.3], [2.7, 1.0], [2.9, 1.3], [3.4, 0.2], [3.2, 0.2], [4.1, 0.1], [3.5, 0.6], [2.7, 1.4],
          [2.3, 0.3], [2.9, 1.5], [3.1, 1.5], [3.5, 0.2], [2.7, 1.6], [3.3, 0.5], [3.0, 1.4], [3.6, 0.2], [3.0, 1.2],
          [2.8, 1.3], [2.5, 1.1], [3.0, 1.5], [3.1, 0.2], [2.6, 1.0], [2.7, 1.2], [2.2, 1.5], [3.7, 0.4], [3.4, 0.2],
          [3.5, 0.3], [3.6, 0.1], [2.5, 1.5], [2.6, 1.2], [2.8, 1.3], [3.1, 0.1], [2.4, 1.0], [3.1, 1.5], [2.3, 1.3],
          [2.8, 1.5], [3.0, 0.3], [3.0, 0.2], [2.5, 1.1], [3.0, 1.5], [3.2, 1.8], [3.9, 0.4], [2.8, 1.4], [4.2, 0.2],
          [3.4, 0.2], [2.7, 1.3], [3.8, 0.3], [3.0, 1.4], [2.6, 1.2], [4.4, 0.4], [3.8, 0.4], [3.1, 0.2], [3.0, 0.1],
          [3.0, 1.5]]
data_y = [1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, 1,
          -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, 1, 1, -1, 1, -1, 1,
          -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1,
          1, 1, -1, -1, -1, -1, 1]

data_x = [x + [1] for x in data_x]
x_train = np.array(data_x)            #обучающая выборка по x
y_train = np.array(data_y)            #обучающая выборка по y

n_dot=len(y_train)                    # общее число точек
n_dot_class1 = 0                      # число точек, принадлежащих классу 1
n_dot_class2 = 0                      # число точек, принадлежащих классу 2
# определение количества точек, принадлежащих к каждому классу
for i in range(n_dot):
    if y_train[i] == 1:
        n_dot_class1 +=1
    else:
        n_dot_class2 +=1

n_train = len(x_train)  # объем (длина) обучающей выборки
w = [0.0, 0.0, 1.0]  # начальные весовые коэффициенты
nt = 0.0005  # шаг сходимости SGD
lm = 0.01  # скорость "забывания" для Q
N = 100000  # число итераций SGD

Q = np.mean([func_sigmoid(w, x, y) for x, y in zip(x_train, y_train)])  # показатель качества
Q_plot = [Q]

for i in range(N):
    k = np.random.randint(0, n_train - 1)  # случайный индекс
    ek = func_sigmoid(w, x_train[k], y_train[k])  # вычисление потерь для выбранного вектора
    w = w - nt * func_dsigmoid(w, x_train[k], y_train[k])  # корректировка весов по SGD
    Q = lm * ek + (1 - lm) * Q  # пересчет показателя качества
    Q_plot.append(Q)

print('===========================Стохастический градиентный спуск SGD ')

print("Общее количество точек n = %d" % (n_dot))
print("Количество точек, относящихся к классу 1 = %d" % (n_dot_class1))
print("Количество точек, относящихся к классу 2 = %d" % (n_dot_class2))
print("Число итераций: N = %d" % (N))
print('Полученные коэффициенты линейной модели w:')
print(w)



max_x1_int = round(max(x_train[:, 0])) + 2 # максимальное округленное целое значение х1 в выборке (с запасом)
max_x1_float = round(max(x_train[:, 0]),1) + 0.1 # максимальное овещественное целое значение х1 в выборке (с запасом)
max_x2_float = round(max(x_train[:, 1]),1) + 0.1 # максимальное овещественное целое значение х2 в выборке (с запасом)
line_x = list(range(max_x1_int))  # формирование графика разделяющей линии
line_y = [-x * w[0] / w[1] - w[2] / w[1] for x in line_x]

x_0 = x_train[y_train == 1]  # формирование точек для 1-го
x_1 = x_train[y_train == -1]  # и 2-го классов

plt.figure(1)                   #первый график
plt.scatter(x_0[:, 0], x_0[:, 1], marker = '^', color='red', label = 'Класс 1')
plt.scatter(x_1[:, 0], x_1[:, 1], color='blue', label = 'Класс 2')
plt.plot(line_x, line_y, color='green', label = 'Разделяющая линия')

plt.xlim([0, max_x1_float])
plt.ylim([0, max_x2_float])
plt.ylabel("x2")
plt.xlabel("x1")
plt.title("Результаты метода стохастического градиентного спуска SGD")
plt.legend()
plt.grid(True)
plt.show()


line_x2 = list(range(N + 1))  # формирование линий второго графика
line_y2 = Q_plot

plt.figure(2)                  #второй график (график ошибки)
plt.xlim([0, N])
plt.ylim([0, 1])
plt.ylabel("Q")
plt.xlabel("t")
plt.title("График показателя качества")
plt.plot(line_x2, line_y2, color='blue')
plt.grid(True)
plt.show()
